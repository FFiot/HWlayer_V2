{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5917bd",
   "metadata": {
    "papermill": {
     "duration": 0.026304,
     "end_time": "2021-11-04T08:48:15.447136",
     "exception": false,
     "start_time": "2021-11-04T08:48:15.420832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## BiLSTM : 19th Plath Best Single Model [ CV:0.1302 LB:0.1147 ]\n",
    "\n",
    "\n",
    "### Abstract\n",
    "1. Feature Engineering\n",
    "    - Logarithmized u_in from the beginning\n",
    "    - First and second derivative for u_in\n",
    "    - RobustScaler fit features with u_out==0 only\n",
    "\n",
    "\n",
    "2. Model\n",
    "    - BiLSTM with ResNet-like structure\n",
    "    - Custom MAE Loss\n",
    "    - optimizer : AdamW\n",
    "    - scheduler : ReduceLrOnPlateum\n",
    "\n",
    "\n",
    "3. Post-Processing\n",
    "    - We used [Chris's PP](https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153)\n",
    "\n",
    "\n",
    "### Common pipeline between Colab and kaggle\n",
    "\n",
    "1. The exact same code can be used between google colab & kaggle \n",
    "\n",
    "2. You can use the kaggle api to automatically load data and submit, and upload datasets\n",
    "\n",
    "3. Parallel fold training with multiple sessions is possible on colab\n",
    "\n",
    "4. Both TPU and GPU can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a4c1a",
   "metadata": {
    "papermill": {
     "duration": 0.024145,
     "end_time": "2021-11-04T08:48:15.496252",
     "exception": false,
     "start_time": "2021-11-04T08:48:15.472107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5517b7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:48:15.554797Z",
     "iopub.status.busy": "2021-11-04T08:48:15.554193Z",
     "iopub.status.idle": "2021-11-04T08:48:15.562635Z",
     "shell.execute_reply": "2021-11-04T08:48:15.563235Z",
     "shell.execute_reply.started": "2021-11-04T07:59:13.661038Z"
    },
    "papermill": {
     "duration": 0.043507,
     "end_time": "2021-11-04T08:48:15.563509",
     "exception": false,
     "start_time": "2021-11-04T08:48:15.520002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    name_v1 = \"Exp-102-ResBiLSTM-v2-CustomLoss-v1-AdamW-LogUin-30Fold-Seed2025-FineTune\"  \n",
    "    editor = \"masato8823\"\n",
    "    api_path = \"/content/drive/masato8823/kaggle.json\"\n",
    "    finetune = \"Exp-102-ResBiLSTM-v2-CustomLoss-v1-AdamW-LogUin-30Fold-Seed2025\"  # This code is after finetuning\n",
    "    lr = 1e-3\n",
    "    weight_decay = 2e-5\n",
    "    epochs = 512\n",
    "    scheduler = \"ReduceLROnPlateau\"  # : ReduceLROnPlateau: CosineDecayRestarts\n",
    "    early_stop = True\n",
    "    seq_len = 80\n",
    "    masking = False\n",
    "    steps_per_epochs = None\n",
    "    train_batch_size = 256\n",
    "    valid_batch_size = 1024\n",
    "    test_batch_size = 1024\n",
    "    n_fold = 30\n",
    "    trn_fold = list(range(30))\n",
    "    seed = 2025\n",
    "    target_col = \"pressure\"\n",
    "    debug = False\n",
    "\n",
    "    # Colab Env\n",
    "    submit_from_colab = False\n",
    "    upload_from_colab = False\n",
    "    \n",
    "    # Kaggle Env\n",
    "    kaggle_dataset_path = \"../input/exp-102\"\n",
    "\n",
    "if Config.debug:\n",
    "    Config.epochs = 2\n",
    "    Config.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd0364",
   "metadata": {
    "papermill": {
     "duration": 0.024066,
     "end_time": "2021-11-04T08:48:15.611850",
     "exception": false,
     "start_time": "2021-11-04T08:48:15.587784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7d1faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:48:15.663624Z",
     "iopub.status.busy": "2021-11-04T08:48:15.663004Z",
     "iopub.status.idle": "2021-11-04T08:48:29.997924Z",
     "shell.execute_reply": "2021-11-04T08:48:29.997226Z",
     "shell.execute_reply.started": "2021-11-04T07:59:16.651092Z"
    },
    "papermill": {
     "duration": 14.362073,
     "end_time": "2021-11-04T08:48:29.998106",
     "exception": false,
     "start_time": "2021-11-04T08:48:15.636033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed2e588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:48:30.058210Z",
     "iopub.status.busy": "2021-11-04T08:48:30.057523Z",
     "iopub.status.idle": "2021-11-04T08:48:37.017911Z",
     "shell.execute_reply": "2021-11-04T08:48:37.017369Z",
     "shell.execute_reply.started": "2021-11-04T07:59:51.742656Z"
    },
    "papermill": {
     "duration": 6.994208,
     "end_time": "2021-11-04T08:48:37.018074",
     "exception": false,
     "start_time": "2021-11-04T08:48:30.023866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from requests import get\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eaee018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:48:37.076898Z",
     "iopub.status.busy": "2021-11-04T08:48:37.076235Z",
     "iopub.status.idle": "2021-11-04T08:48:37.093464Z",
     "shell.execute_reply": "2021-11-04T08:48:37.092906Z",
     "shell.execute_reply.started": "2021-11-04T08:00:30.32973Z"
    },
    "papermill": {
     "duration": 0.050887,
     "end_time": "2021-11-04T08:48:37.093604",
     "exception": false,
     "start_time": "2021-11-04T08:48:37.042717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Utils\n",
    "# ========================================\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"save log\"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.general_logger = logging.getLogger(path)\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n",
    "        if len(self.general_logger.handlers) == 0:\n",
    "            self.general_logger.addHandler(stream_handler)\n",
    "            self.general_logger.addHandler(file_general_handler)\n",
    "            self.general_logger.setLevel(logging.INFO)\n",
    "\n",
    "    def info(self, message):\n",
    "        # display time\n",
    "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
    "\n",
    "    @staticmethod\n",
    "    def now_string():\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    \n",
    "class Util:\n",
    "    \"\"\"save & load\"\"\"\n",
    "    @classmethod\n",
    "    def dump(cls, value, path):\n",
    "        joblib.dump(value, path, compress=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return joblib.load(path)\n",
    "    \n",
    "    \n",
    "class HorizontalDisplay:\n",
    "    \"\"\"display dataframe\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        template = '<div style=\"float: left; padding: 10px;\">{0}</div>'\n",
    "        return \"\\n\".join(template.format(arg._repr_html_())\n",
    "                         for arg in self.args)\n",
    "    \n",
    "    \n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    dfs = []\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    dfs.append(df[col].astype(np.int8))\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    dfs.append(df[col].astype(np.int16))\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    dfs.append(df[col].astype(np.int32))\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    dfs.append(df[col].astype(np.int64) ) \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    dfs.append(df[col].astype(np.float32))\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    dfs.append(df[col].astype(np.float32))\n",
    "                else:\n",
    "                    dfs.append(df[col].astype(np.float64))\n",
    "        else:\n",
    "            dfs.append(df[col])\n",
    "    \n",
    "    df_out = pd.concat(dfs, axis=1)\n",
    "    if verbose:\n",
    "        end_mem = df_out.memory_usage().sum() / 1024**2\n",
    "        num_reduction = str(100 * (start_mem - end_mem) / start_mem)\n",
    "        print(f'Mem. usage decreased to {str(end_mem)[:3]}Mb:  {num_reduction[:2]}% reduction')\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dac833",
   "metadata": {
    "papermill": {
     "duration": 0.02449,
     "end_time": "2021-11-04T08:48:37.143238",
     "exception": false,
     "start_time": "2021-11-04T08:48:37.118748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SetUp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55ee9e3",
   "metadata": {
    "papermill": {
     "duration": 0.026589,
     "end_time": "2021-11-04T08:49:04.879207",
     "exception": false,
     "start_time": "2021-11-04T08:49:04.852618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4212ef81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:49:04.945732Z",
     "iopub.status.busy": "2021-11-04T08:49:04.945000Z",
     "iopub.status.idle": "2021-11-04T08:55:19.254641Z",
     "shell.execute_reply": "2021-11-04T08:55:19.255442Z",
     "shell.execute_reply.started": "2021-11-04T08:01:14.411887Z"
    },
    "papermill": {
     "duration": 374.349522,
     "end_time": "2021-11-04T08:55:19.255940",
     "exception": false,
     "start_time": "2021-11-04T08:49:04.906418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Database/train.csv')\n",
    "test = pd.read_csv('./Database/test.csv')\n",
    "\n",
    "# fisrt, add fold index\n",
    "train[\"fold\"], test[\"fold\"] = -1, -1\n",
    "for i, lst in enumerate(\n",
    "    model_selection.StratifiedGroupKFold(\n",
    "    n_splits=Config.n_fold, \n",
    "    shuffle=True,\n",
    "    random_state=Config.seed).split(X=train, \n",
    "                                    y=train['R'].astype(str) + train['C'].astype(str), \n",
    "                                    groups=train[\"breath_id\"])):\n",
    "    if i in Config.trn_fold:\n",
    "        train.loc[lst[1].tolist(), \"fold\"] = i\n",
    "    \n",
    "test[Config.target_col] = np.nan\n",
    "\n",
    "# cut seq\n",
    "train = train[train.groupby('breath_id')['time_step'].cumcount() < Config.seq_len].reset_index(drop=True)\n",
    "test = test[test.groupby('breath_id')['time_step'].cumcount() < Config.seq_len].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca7034",
   "metadata": {
    "papermill": {
     "duration": 0.033874,
     "end_time": "2021-11-04T08:55:24.649991",
     "exception": false,
     "start_time": "2021-11-04T08:55:24.616117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cafc2a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:55:24.736904Z",
     "iopub.status.busy": "2021-11-04T08:55:24.720964Z",
     "iopub.status.idle": "2021-11-04T08:55:24.753784Z",
     "shell.execute_reply": "2021-11-04T08:55:24.754329Z",
     "shell.execute_reply.started": "2021-11-04T08:07:58.238021Z"
    },
    "papermill": {
     "duration": 0.070569,
     "end_time": "2021-11-04T08:55:24.754536",
     "exception": false,
     "start_time": "2021-11-04T08:55:24.683967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregation(input_df, group_key, group_values, agg_methods):\n",
    "    \"\"\"ref:https://github.com/pfnet-research/xfeat/blob/master/xfeat/helper.py\"\"\"\n",
    "    new_df = []\n",
    "    for agg_method in agg_methods:\n",
    "        for col in group_values:\n",
    "            if callable(agg_method):\n",
    "                agg_method_name = agg_method.__name__\n",
    "            else:\n",
    "                agg_method_name = agg_method\n",
    "            new_col = f\"agg_{agg_method_name}_{col}_grpby_{group_key}\"\n",
    "            df_agg = (input_df[[col] + [group_key]].groupby(group_key)[[col]].agg(agg_method))\n",
    "            df_agg.columns = [new_col]\n",
    "            new_df.append(df_agg)\n",
    "            \n",
    "    _df = pd.concat(new_df, axis=1).reset_index()\n",
    "    output_df = pd.merge(input_df[[group_key]], _df, on=group_key, how=\"left\")\n",
    "    return output_df.drop(group_key, axis=1)\n",
    "\n",
    "def get_raw_features(input_df):\n",
    "    cols = [\n",
    "        \"time_step\",\n",
    "        \"u_in\",\n",
    "    ]\n",
    "    output_df = input_df[cols].copy()\n",
    "    return output_df\n",
    "\n",
    "def get_ohe_features(input_df):\n",
    "    cols = [\"R\", \"C\"]\n",
    "    encoder = ce.OneHotEncoder()\n",
    "    output_df = encoder.fit_transform(input_df[cols].astype(str))\n",
    "    return output_df\n",
    "\n",
    "def get_fold_value(input_df):\n",
    "    return input_df[[\"fold\"]]\n",
    "\n",
    "def get_target_value(input_df):\n",
    "    return input_df[[Config.target_col]]\n",
    "\n",
    "def get_cumlative_grpby_breath_id_features(input_df):\n",
    "    \n",
    "    input_df[\"area\"] = input_df[\"time_step\"] * input_df[\"u_in\"]\n",
    "    group_key = \"breath_id\"\n",
    "    group_values = [\"u_in\", \"area\"]\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    for group_val in group_values:\n",
    "        col_name = f\"agg_cumsum_{group_val}_grpby_{group_key}\"\n",
    "        output_df[col_name] = input_df.groupby(group_key)[group_val].cumsum()\n",
    "    \n",
    "    # tubotubo feats\n",
    "    output_df[\"divede_cumsum_u_in_by_time_step\"] = np.log1p(output_df[\"agg_cumsum_u_in_grpby_breath_id\"] /\n",
    "                                                    (input_df[\"time_step\"] + 1e-2))\n",
    "        \n",
    "    return output_df.fillna(0)\n",
    "\n",
    "\n",
    "def get_mask_feature(input_df):\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"mask\"] = input_df[\"u_out\"] == 0\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def _get_agg_col_name(group_key, group_values, agg_methods):\n",
    "    out_cols = []\n",
    "    for group_val in group_values:\n",
    "        for agg_method in agg_methods:\n",
    "            out_cols.append(f\"agg_{agg_method}_{group_val}_grpby_{group_key}\")\n",
    "    return out_cols\n",
    "\n",
    "\n",
    "def get_shift_grpby_breath_id_features(input_df):\n",
    "    shift_times = [1, 2, 3, 4]\n",
    "    group_key = \"breath_id\"\n",
    "    group_values = [\"u_in\"]\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    for t in shift_times:\n",
    "        _df = input_df.groupby(group_key)[group_values].shift(t)\n",
    "        _df.columns = [f'shift={t}_{col}_grpby_{group_key}' for col in group_values]\n",
    "        output_df = pd.concat([output_df, _df], axis=1)\n",
    "    return output_df.fillna(0)\n",
    "\n",
    "\n",
    "def get_diff_grpby_breath_id_features(input_df):\n",
    "    diff_times = [1, 2, 3, 4]\n",
    "    group_key = \"breath_id\"\n",
    "    group_values = [\"time_step\", \"u_in\"]\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"breath_id\"] = input_df[\"breath_id\"].copy()\n",
    "\n",
    "    for t in diff_times:\n",
    "        _df = input_df.groupby(group_key)[group_values].diff(t)\n",
    "        _df.columns = [f'diff={t}_{col}_grpby_{group_key}' for col in group_values]\n",
    "        output_df = pd.concat([output_df, _df], axis=1)\n",
    "\n",
    "    # 1st derivative\n",
    "    for n in [1]:\n",
    "        col = f'slope={n}_time_step_u_in'\n",
    "        val = (output_df[f'diff={n}_u_in_grpby_breath_id'] /\n",
    "               (output_df[f'diff={n}_time_step_grpby_breath_id'] + 1e-8))\n",
    "        output_df[col] = val\n",
    "               \n",
    "    return output_df.fillna(0).drop(\"breath_id\", axis=1)\n",
    "\n",
    "\n",
    "def get_accel_grpby_breath_id_feature(input_df):\n",
    "    grby_u_in = input_df.groupby('breath_id')['u_in']\n",
    "    grby_time_step = input_df.groupby('breath_id')['time_step']\n",
    "    \n",
    "    # 2nd derivative\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"accel\"] = ((grby_u_in.shift(0) - 2 * grby_u_in.shift(1) + grby_u_in.shift(2)) /\n",
    "                 (grby_time_step.diff(1) * grby_time_step.diff(1).shift(1)))\n",
    "\n",
    "    # clip accel\n",
    "    p001 = output_df[\"accel\"].quantile(0.01)\n",
    "    p099 = output_df[\"accel\"].quantile(0.99)\n",
    "    output_df[\"accel\"] = output_df[\"accel\"].clip(p001, p099)\n",
    "    \n",
    "    return output_df.fillna(0)\n",
    "\n",
    "\n",
    "def get_features_df(train, test):\n",
    "    whole_df = pd.concat([train, test]).reset_index(drop=True)\n",
    "    whole_df[\"u_in\"] = np.log1p(whole_df[\"u_in\"])  \n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    funcs = [\n",
    "        get_target_value,\n",
    "        get_mask_feature,\n",
    "        get_fold_value,\n",
    "        get_raw_features,\n",
    "        get_ohe_features,\n",
    "        get_cumlative_grpby_breath_id_features,\n",
    "        get_shift_grpby_breath_id_features,\n",
    "        get_diff_grpby_breath_id_features,\n",
    "        get_accel_grpby_breath_id_feature,\n",
    "    ]\n",
    "\n",
    "    for func in funcs:\n",
    "        print(func.__name__)\n",
    "        _df = func(whole_df)\n",
    "        if func.__name__ not in [\n",
    "                                 \"get_mask_feature\",\n",
    "                                 \"get_target_value\",\n",
    "                                 \"get_fold_value\" \n",
    "                                 ]:\n",
    "\n",
    "            scaler = RobustScaler()\n",
    "            _df[whole_df[\"u_out\"]==0] = scaler.fit_transform(_df[whole_df[\"u_out\"]==0])\n",
    "            _df[whole_df[\"u_out\"]==1] = scaler.transform(_df[whole_df[\"u_out\"]==1])\n",
    "            _df = reduce_mem_usage(_df)\n",
    "            \n",
    "        output_df = pd.concat([output_df, _df], axis=1)\n",
    "    \n",
    "    output_df[\"breath_id\"] = whole_df[\"breath_id\"].copy()\n",
    "    train_feats_df = output_df.iloc[:len(train)]\n",
    "    test_feats_df = output_df.iloc[len(train):].reset_index(drop=True)\n",
    "    \n",
    "    return train_feats_df, test_feats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5c97f",
   "metadata": {
    "papermill": {
     "duration": 0.034233,
     "end_time": "2021-11-04T08:55:24.821765",
     "exception": false,
     "start_time": "2021-11-04T08:55:24.787532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459c0941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:55:24.892253Z",
     "iopub.status.busy": "2021-11-04T08:55:24.891535Z",
     "iopub.status.idle": "2021-11-04T08:55:24.912440Z",
     "shell.execute_reply": "2021-11-04T08:55:24.912861Z",
     "shell.execute_reply.started": "2021-11-04T08:07:59.018464Z"
    },
    "papermill": {
     "duration": 0.057805,
     "end_time": "2021-11-04T08:55:24.913030",
     "exception": false,
     "start_time": "2021-11-04T08:55:24.855225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_mae_loss(y_true, y_pred, loss_mask):\n",
    "    y_true_0 = tf.boolean_mask(y_true, tf.cast(loss_mask, dtype=tf.bool))\n",
    "    y_pred_0 = tf.boolean_mask(y_pred, tf.cast(loss_mask, dtype=tf.bool))\n",
    "    score = tf.keras.losses.mean_absolute_error(y_true_0, y_pred_0)\n",
    "    return score\n",
    "\n",
    "def custom_mae_loss(y_true, y_pred, loss_mask):\n",
    "    y_true_0 = tf.boolean_mask(y_true, tf.cast(loss_mask, dtype=tf.bool))\n",
    "    y_pred_0 = tf.boolean_mask(y_pred, tf.cast(loss_mask, dtype=tf.bool))\n",
    "    score_0 = tf.keras.losses.mean_absolute_error(y_true_0, y_pred_0)\n",
    "\n",
    "    y_true_1 = tf.boolean_mask(y_true, tf.cast(1 - loss_mask, dtype=tf.bool))\n",
    "    y_pred_1 = tf.boolean_mask(y_pred, tf.cast(1 - loss_mask, dtype=tf.bool))\n",
    "    score_1 = tf.keras.losses.mean_absolute_error(y_true_1, y_pred_1)\n",
    "    score = score_0 * 2 + score_1 * 1\n",
    "    return score\n",
    "\n",
    "\n",
    "def build_model(input_shape, only_inference=False):\n",
    "    \n",
    "    inputs_x = tf.keras.layers.Input(shape=input_shape, name=\"input_x\")\n",
    "    inputs_y = tf.keras.layers.Input(shape=(input_shape[0], 1), name=\"input_y\")\n",
    "    inputs_w = tf.keras.layers.Input(shape=(input_shape[0], 1), name=\"input_w\")\n",
    "\n",
    "    x0 = Bidirectional(LSTM(512, return_sequences=True))(inputs_x)\n",
    "    x = tf.keras.layers.Concatenate(axis=2)([inputs_x, x0])\n",
    "\n",
    "    x1 = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Concatenate(axis=2)([x0, x1])\n",
    "\n",
    "    x2 = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Concatenate(axis=2)([x1, x2])\n",
    "\n",
    "    x3 = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Concatenate(axis=2)([x2, x3])\n",
    "\n",
    "    x4 = Bidirectional(LSTM(512, return_sequences=True))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate(axis=2)([x0, x1, x2, x3, x4])\n",
    "    x = tf.keras.layers.Dense(64, activation=\"selu\")(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "    model = tf.keras.Model(inputs=[inputs_x, inputs_y, inputs_w], outputs=x)\n",
    "    \n",
    "    if not only_inference:\n",
    "        model.add_loss(custom_mae_loss(inputs_y, x, inputs_w))\n",
    "        model.add_metric(masked_mae_loss(inputs_y, x, inputs_w), name=\"masked_mae\")\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(lr=Config.lr, weight_decay=Config.weight_decay)\n",
    "    model.compile(optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(input_shape, only_inference=False):\n",
    "    # TPU setting\n",
    "    if TPU:\n",
    "        if COLAB:  # tpu in colab\n",
    "            tf.config.experimental_connect_to_cluster(TPU)\n",
    "            tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "            tpu_strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
    "\n",
    "        else:  # tpu in kaggle kernel\n",
    "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "            tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "        with tpu_strategy.scope():\n",
    "            model = build_model(input_shape, only_inference)\n",
    "\n",
    "    else:\n",
    "        model = build_model(input_shape, only_inference)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d0a51",
   "metadata": {
    "papermill": {
     "duration": 0.033216,
     "end_time": "2021-11-04T08:55:24.979736",
     "exception": false,
     "start_time": "2021-11-04T08:55:24.946520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb63bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:55:25.078575Z",
     "iopub.status.busy": "2021-11-04T08:55:25.050797Z",
     "iopub.status.idle": "2021-11-04T08:55:25.088948Z",
     "shell.execute_reply": "2021-11-04T08:55:25.088349Z",
     "shell.execute_reply.started": "2021-11-04T08:08:00.266709Z"
    },
    "papermill": {
     "duration": 0.075559,
     "end_time": "2021-11-04T08:55:25.089101",
     "exception": false,
     "start_time": "2021-11-04T08:55:25.013542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NO_FEATURES = [\"breath_id\", \"mask\", \"fold\", Config.target_col]\n",
    "\n",
    "# ========================================\n",
    "# DataLoader\n",
    "# ========================================\n",
    "def get_dataset(X, y=None, dataset=\"test\"):\n",
    "    \n",
    "    if dataset==\"train\":\n",
    "        train_dataset = (\n",
    "            tf.data.Dataset\n",
    "            .from_tensor_slices((X, y))\n",
    "            .shuffle(10**8)\n",
    "            .batch(Config.train_batch_size)\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "            )\n",
    "        if Config.steps_per_epochs is not None:\n",
    "            train_dataset = train_dataset.repeat()\n",
    "        return train_dataset\n",
    "\n",
    "    elif dataset==\"valid\":\n",
    "        valid_dataset = (\n",
    "            tf.data.Dataset\n",
    "            .from_tensor_slices((X, y))\n",
    "            .batch(Config.valid_batch_size)\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "        return valid_dataset\n",
    "    \n",
    "    elif dataset==\"test\":\n",
    "        test_dataset = (\n",
    "            tf.data.Dataset\n",
    "            .from_tensor_slices(X)\n",
    "            .batch(Config.test_batch_size)\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "        return test_dataset\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def transform_features_for_rnn(input_df, feature_names='__all__'):\n",
    "    if feature_names == \"__all__\":\n",
    "        feature_names = input_df.columns.tolist()\n",
    "\n",
    "    features = []\n",
    "    _input_df = input_df.copy()\n",
    "    _input_df['time_index'] = _input_df.groupby('breath_id')['time_step'].cumcount()\n",
    "    _input_df = _input_df.query('time_index < @Config.seq_len' ).reset_index(drop=True)\n",
    "    use_cols = list(set(feature_names) - set(['breath_id', 'mask', 'fold']))\n",
    "    \n",
    "    if Config.masking:\n",
    "        # pad 0 in u_out==1\n",
    "        _input_df.loc[~input_df[\"mask\"], use_cols] = -1\n",
    "\n",
    "    pdf = pd.pivot_table(data=_input_df, index='breath_id', columns='time_index')\n",
    "    breath_id_count = len(pdf)\n",
    "\n",
    "    for feat_name in feature_names:\n",
    "        if feat_name in NO_FEATURES:\n",
    "            continue\n",
    "            \n",
    "        _feat = pdf[feat_name].values.reshape(breath_id_count, -1, 1)\n",
    "        features.append(_feat)\n",
    "\n",
    "    features = np.concatenate(features, axis=2)\n",
    "    print(features.shape)\n",
    "    return features\n",
    "    \n",
    "\n",
    "def transform_target_for_rnn(train, target_col=Config.target_col):\n",
    "    _train = train.copy()\n",
    "    _train['time_index'] = _train.groupby('breath_id')['time_step'].cumcount()\n",
    "    _train = _train.query('time_index < @Config.seq_len').reset_index(drop=True)\n",
    "\n",
    "    if target_col != 'mask':\n",
    "        if Config.masking:\n",
    "            _train.loc[~train['mask'], Config.target_col] = -1\n",
    "\n",
    "    pdf = pd.pivot_table(data=_train, index='breath_id', columns='time_index', values=[target_col])\n",
    "    breath_id_count = len(pdf)\n",
    "    target = pdf[target_col].values.reshape(breath_id_count, -1, 1)\n",
    "    return target\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Scheduler\n",
    "# ========================================\n",
    "def get_scheduler(monitor):\n",
    "    if Config.scheduler == \"custom-v1\":\n",
    "        def custom_scheduler(epoch):\n",
    "            x = Config.lr\n",
    "            if epoch >= 125: x = 0.0007\n",
    "            if epoch >= 185: x = 0.0004\n",
    "            if epoch >= 250: x = 0.0003\n",
    "            if epoch >= 275: x = 0.0002\n",
    "            if epoch >= 290: x = 0.00015\n",
    "            if epoch >= 305: x = 0.0001\n",
    "            if epoch >= 320: x = 0.000075\n",
    "            if epoch >= 325: x = 0.00006\n",
    "            if epoch >= 330: x = 0.00004\n",
    "            if epoch >= 330: x = 0.00003\n",
    "            if epoch >= 340: x = 0.00002\n",
    "            if epoch >= 345: x = 0.00001\n",
    "            return x\n",
    "        \n",
    "        # plot steps\n",
    "        plt.plot([custom_scheduler(i) for i in range(Config.epochs)])\n",
    "        plt.show()\n",
    "        scheduler = tf.keras.callbacks.LearningRateScheduler(custom_scheduler, verbose=1)\n",
    "    \n",
    "    elif Config.scheduler == \"ReduceLROnPlateau\":\n",
    "        scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=monitor, \n",
    "            factor=0.7,\n",
    "            patience=16,\n",
    "            min_lr=1e-7, \n",
    "            verbose=1\n",
    "            )\n",
    "        \n",
    "    elif Config.scheduler == \"CosineDecayRestarts\":\n",
    "        cisine_decay_r = tf.keras.experimental.CosineDecayRestarts(\n",
    "            Config.lr,\n",
    "            first_decay_steps=Config.epochs // 2,\n",
    "            t_mul=1,\n",
    "            m_mul=1,\n",
    "            alpha=0.01\n",
    "            )\n",
    "        \n",
    "        # plot steps\n",
    "        plt.plot([cisine_decay_r(i) for i in range(Config.epochs)])\n",
    "        plt.show()\n",
    "\n",
    "        scheduler = tf.keras.callbacks.LearningRateScheduler(cisine_decay_r, verbose=1)\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "# ========================================\n",
    "# Training & Inference Func\n",
    "# ========================================\n",
    "def training_rnn(train_df, valid_df, model, filepath):\n",
    "    \"\"\"Training func for RNN\"\"\"\n",
    "\n",
    "    train_x = transform_features_for_rnn(train_df)\n",
    "    valid_x = transform_features_for_rnn(valid_df)\n",
    "    train_y = transform_target_for_rnn(train_df)\n",
    "    valid_y = transform_target_for_rnn(valid_df)\n",
    "    \n",
    "    train_w = transform_target_for_rnn(train_df, \"mask\")\n",
    "    valid_w = transform_target_for_rnn(valid_df, \"mask\")\n",
    "    train_inputs = {\"input_x\":train_x, \"input_y\":train_y, \"input_w\":train_w}\n",
    "    valid_inputs = {\"input_x\":valid_x, \"input_y\":valid_y, \"input_w\":valid_w}\n",
    "    \n",
    "    tr_dataset = get_dataset(X=train_inputs, y=train_y, dataset=\"train\")\n",
    "    va_dataset = get_dataset(X=valid_inputs, y=valid_y, dataset=\"valid\")\n",
    "    \n",
    "    monitor = \"val_masked_mae\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath, \n",
    "        monitor=monitor, \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        mode=\"min\")\n",
    "    \n",
    "    callbacks = [checkpoint, get_scheduler(monitor)]\n",
    "    \n",
    "    if Config.early_stop:\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=monitor,\n",
    "            min_delta=0.0,\n",
    "            patience=36, \n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks += [early_stop]\n",
    "\n",
    "    model.fit(\n",
    "        tr_dataset, \n",
    "        epochs=Config.epochs, \n",
    "        verbose=1, \n",
    "        callbacks=callbacks,\n",
    "        validation_data=va_dataset, \n",
    "        steps_per_epoch=Config.steps_per_epochs,\n",
    "    )\n",
    "\n",
    "    \n",
    "def inference_rnn(test_df, model, filepath, is_test=False):\n",
    "    \"\"\"Inference func for RNN\"\"\"\n",
    "    model.load_weights(filepath)\n",
    "    \n",
    "    test_x = transform_features_for_rnn(test_df) \n",
    "    test_y_dummy = transform_target_for_rnn(test_df, \"mask\")\n",
    "    test_w = transform_target_for_rnn(test_df, \"mask\")\n",
    "    test_inputs = {\"input_x\":test_x, \"input_y\":test_y_dummy, \"input_w\":test_w}\n",
    "    \n",
    "    te_dataset = get_dataset(X=test_inputs, y=None, dataset=\"test\")\n",
    "    preds = model.predict(te_dataset)\n",
    "    print(preds.shape)\n",
    "    pad_width = 80 - Config.seq_len\n",
    "\n",
    "    if is_test:\n",
    "        preds = np.pad(preds, pad_width=[(0, 0), (0, pad_width), (0, 0)])\n",
    "\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    return preds.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "880203e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:55:25.180915Z",
     "iopub.status.busy": "2021-11-04T08:55:25.180244Z",
     "iopub.status.idle": "2021-11-04T08:55:25.183169Z",
     "shell.execute_reply": "2021-11-04T08:55:25.182579Z",
     "shell.execute_reply.started": "2021-11-04T08:08:02.589804Z"
    },
    "papermill": {
     "duration": 0.060141,
     "end_time": "2021-11-04T08:55:25.183316",
     "exception": false,
     "start_time": "2021-11-04T08:55:25.123175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred, mask=None):\n",
    "    if mask is not None:\n",
    "        y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    score = mean_absolute_error(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def train_cv_rnn(train, metrics, name, directory):\n",
    "    \"\"\"cross validation training for RNN\"\"\"\n",
    "    input_shape = (Config.seq_len, train.shape[1] - len(NO_FEATURES)) \n",
    "    oof = np.zeros(len(train))\n",
    "    for i_fold in range(Config.n_fold):\n",
    "        \n",
    "        if i_fold in Config.trn_fold:\n",
    "        \n",
    "            K.clear_session()\n",
    "            filepath = os.path.join(directory, f\"{name}_fold{i_fold+1}.h5\")\n",
    "            tr_df, va_df = (train[train[\"fold\"] != i_fold].reset_index(drop=True),\n",
    "                            train[train[\"fold\"] == i_fold].reset_index(drop=True))\n",
    "\n",
    "            if not os.path.isfile(filepath):  # if trained model, no training\n",
    "                model = get_model(input_shape)\n",
    "                if Config.finetune is not None:  # fine tune (additinal training) \n",
    "\n",
    "                    if COLAB:\n",
    "                        pretrain_filepath = os.path.join(\n",
    "                            OUTPUT, Config.finetune, \"model\", Config.finetune + f\"-{Config.seed}_fold{i_fold+1}.h5\"\n",
    "                            )\n",
    "                    else:\n",
    "                        pretrain_filepath = os.path.join(\n",
    "                            \"../input\", Config.finetune, \"model\", Config.finetune + f\"-{Config.seed}_fold{i_fold+1}.h5\"\n",
    "                            )\n",
    "                                                         \n",
    "                    model.load_weights(pretrain_filepath)\n",
    "                \n",
    "                training_rnn(tr_df, va_df, model, filepath)\n",
    "  \n",
    "            K.clear_session()\n",
    "            model = get_model(input_shape, only_inference=True)\n",
    "            preds = inference_rnn(va_df, model, filepath)\n",
    "            score = metrics(np.array(va_df[Config.target_col]), np.array(preds), mask=np.array(va_df[\"mask\"], dtype=bool))\n",
    "            logger.info(f\"{name}_fold{i_fold+1} >>> val socre:{score:.4f}\")\n",
    "            oof[train[\"fold\"] == i_fold] = preds\n",
    "    \n",
    "    score = metrics(np.array(train[Config.target_col]), oof,  mask=np.array(train[\"mask\"], dtype=bool))\n",
    "    logger.info(f\"{name} >>> val score:{score:.4f}\")\n",
    "    return oof\n",
    "\n",
    "\n",
    "def predict_cv_rnn(test, name, directory, is_test=True):\n",
    "    \"\"\"cross varidation prediction for RNN\"\"\"\n",
    "    input_shape = (Config.seq_len, test.shape[1] - len(NO_FEATURES))\n",
    "    model = get_model(input_shape, only_inference=True)\n",
    "    preds_fold = []\n",
    "    preds_fold_df = pd.DataFrame()\n",
    "    for i_fold in range(Config.n_fold):\n",
    "        if i_fold in Config.trn_fold:\n",
    "            \n",
    "            filepath = os.path.join(directory, f\"{name}_fold{i_fold+1}.h5\")\n",
    "            preds = inference_rnn(test, model, filepath, is_test=is_test)\n",
    "            preds_fold.append(preds)\n",
    "            preds_fold_df[f\"fold={i_fold:02}\"] = preds\n",
    "            logger.info(f\"{name}_fold{i_fold+1} inference\")\n",
    "    \n",
    "    preds = np.median(preds_fold, axis=0)\n",
    "    preds_fold_df.to_csv(os.path.join(EXP_PREDS, \"preds_fold.csv\"), index=False)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def post_processing(prediction, train_target):\n",
    "    \"\"\"round & clipping post process (by chirs)\"\"\"\n",
    "    \n",
    "    min_target, max_target = np.min(train_target), np.max(train_target)\n",
    "        \n",
    "    unique_target = np.array(sorted(np.unique(train_target)))\n",
    "    target_step = unique_target[1] - unique_target[0] \n",
    "\n",
    "    output_target = np.round((prediction - min_target) / target_step) * target_step + min_target\n",
    "    output_target = np.clip(output_target, min_target, max_target)\n",
    "\n",
    "    return output_target\n",
    "\n",
    "\n",
    "def plot_regression_result(y, oof, directory):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.distplot(y, label='y', color='cyan', ax=ax)\n",
    "    sns.distplot(oof, label='oof', color=\"magenta\", ax=ax)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_title(\"regression_result\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b7f30",
   "metadata": {
    "papermill": {
     "duration": 0.034256,
     "end_time": "2021-11-04T08:55:25.251589",
     "exception": false,
     "start_time": "2021-11-04T08:55:25.217333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c04dc1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T08:55:25.333553Z",
     "iopub.status.busy": "2021-11-04T08:55:25.332502Z",
     "iopub.status.idle": "2021-11-04T09:29:30.757037Z",
     "shell.execute_reply": "2021-11-04T09:29:30.757554Z",
     "shell.execute_reply.started": "2021-11-04T08:08:05.037756Z"
    },
    "papermill": {
     "duration": 2045.472321,
     "end_time": "2021-11-04T09:29:30.757828",
     "exception": false,
     "start_time": "2021-11-04T08:55:25.285507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ============= # Preprocess # ============= #\n",
      "get_target_value\n",
      "get_mask_feature\n",
      "get_fold_value\n",
      "get_raw_features\n",
      "Mem. usage decreased to 76.Mb:  49% reduction\n",
      "get_ohe_features\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ce' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13784/1982691600.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# preprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"# ============= # Preprocess # ============= #\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_feats_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_feats_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_features_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_feats_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_feats_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_feats_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13784/2320644226.py\u001b[0m in \u001b[0;36mget_features_df\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhole_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         if func.__name__ not in [\n\u001b[0;32m    145\u001b[0m                                  \u001b[1;34m\"get_mask_feature\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13784/2320644226.py\u001b[0m in \u001b[0;36mget_ohe_features\u001b[1;34m(input_df)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_ohe_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"R\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0moutput_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ce' is not defined"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "print(\"# ============= # Preprocess # ============= #\")\n",
    "train_feats_df, test_feats_df = get_features_df(train, test)\n",
    "display(train_feats_df)\n",
    "print(train_feats_df.shape, test_feats_df.shape)\n",
    "\n",
    "# training\n",
    "print(\"# ============= # Training # ============= #\")\n",
    "oof_df = pd.DataFrame()\n",
    "name = f\"{Config.name_v1}-{Config.seed}\"\n",
    "oof = train_cv_rnn(\n",
    "    train=train_feats_df,\n",
    "    metrics=metrics, \n",
    "    name=name, \n",
    "    directory=EXP_MODEL)\n",
    "\n",
    "oof_df[name] = oof\n",
    "oof_df.to_csv(os.path.join(EXP_PREDS, \"oof.csv\"), index=False)\n",
    "\n",
    "\n",
    "# get oof score \n",
    "y_true = train[Config.target_col]\n",
    "y_pred = oof_df.median(axis=1)\n",
    "mask = train[\"u_out\"] == 0\n",
    "used_mask = train[\"fold\"].isin(Config.trn_fold)\n",
    "\n",
    "oof_score = metrics(y_true[used_mask], y_pred[used_mask], mask=mask)  # compe metrics \n",
    "oof_score_all = metrics(y_true[used_mask], y_pred[used_mask])  # all record score\n",
    "logger.info(f\"{Config.name_v1} compe score:{oof_score:.4f}, all score:{oof_score_all}\")\n",
    "\n",
    "\n",
    "# post processing for OOF\n",
    "print(\"# ============= # PP for OOF # ============= #\")\n",
    "y_pred = post_processing(prediction=y_pred, train_target=y_true)\n",
    "\n",
    "oof_score = metrics(y_true[used_mask], y_pred[used_mask], mask=mask)   # compe metrics \n",
    "oof_score_all = metrics(y_true[used_mask], y_pred[used_mask])  # all record score\n",
    "logger.info(f\"{Config.name_v1} compe score pp:{oof_score:.4f}, all score pp:{oof_score_all}\")\n",
    "\n",
    "\n",
    "# inference\n",
    "print(\"# ============= # Inference # ============= #\")\n",
    "preds_df = pd.DataFrame()\n",
    "name = f\"{Config.name_v1}-{Config.seed}\"\n",
    "preds = predict_cv_rnn(\n",
    "    test=test_feats_df,\n",
    "    name=name, \n",
    "    directory=EXP_MODEL\n",
    ")\n",
    "preds_df[name] = preds\n",
    "\n",
    "preds_df.to_csv(os.path.join(EXP_PREDS, \"preds.csv\"), index=False)\n",
    "test_pred = preds_df.median(axis=1)\n",
    "\n",
    "# post processing for OOF\n",
    "print(\"# ============= # PP for PREDS # ============= #\")\n",
    "test_pred = post_processing(prediction=test_pred, train_target=y_true)\n",
    "\n",
    "sample_submission['pressure'] = test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611aadd",
   "metadata": {
    "papermill": {
     "duration": 0.106036,
     "end_time": "2021-11-04T09:29:30.970518",
     "exception": false,
     "start_time": "2021-11-04T09:29:30.864482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c796f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T09:29:31.186825Z",
     "iopub.status.busy": "2021-11-04T09:29:31.186193Z",
     "iopub.status.idle": "2021-11-04T09:31:03.355389Z",
     "shell.execute_reply": "2021-11-04T09:31:03.355906Z",
     "shell.execute_reply.started": "2021-11-04T08:39:51.114158Z"
    },
    "papermill": {
     "duration": 92.278735,
     "end_time": "2021-11-04T09:31:03.356113",
     "exception": false,
     "start_time": "2021-11-04T09:29:31.077378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plot_regression_result(y_true[(used_mask*mask).astype(bool)],\n",
    "                             y_pred[(used_mask*mask).astype(bool)], \n",
    "                             directory=EXP_FIG)\n",
    "fig.savefig(os.path.join(EXP_FIG, \"regression_result.png\"), dpi=300)\n",
    "\n",
    "train[\"oof\"] = y_pred\n",
    "\n",
    "# plot oof score (good & bad)\n",
    "train_ = train[(used_mask*mask).astype(bool)].reset_index(drop=True)\n",
    "oof_score_by_breath =train_.groupby(\"breath_id\").apply(lambda x: metrics(x[\"pressure\"], x[\"oof\"], x[\"u_out\"]==0))\n",
    "\n",
    "# best12\n",
    "breath_id_for_plot = oof_score_by_breath.sort_values().index[list(range(12))]\n",
    "fig = plot_pressure_line(train_, breath_ids=list(breath_id_for_plot), additional_cols=[\"oof\"])\n",
    "fig.savefig(os.path.join(EXP_FIG, \"best_oof.png\"), dpi=300)\n",
    "\n",
    "# bad12\n",
    "breath_id_for_plot = oof_score_by_breath.sort_values(ascending=False).index[list(range(12))]\n",
    "fig = plot_pressure_line(train_, breath_ids=list(breath_id_for_plot), additional_cols=[\"oof\"])\n",
    "fig.savefig(os.path.join(EXP_FIG, \"bad_oof.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f657b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T09:31:03.609599Z",
     "iopub.status.busy": "2021-11-04T09:31:03.608816Z",
     "iopub.status.idle": "2021-11-04T09:31:03.620582Z",
     "shell.execute_reply": "2021-11-04T09:31:03.621145Z",
     "shell.execute_reply.started": "2021-11-04T08:41:15.311452Z"
    },
    "papermill": {
     "duration": 0.14046,
     "end_time": "2021-11-04T09:31:03.621324",
     "exception": false,
     "start_time": "2021-11-04T09:31:03.480864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_oof_score(train):\n",
    "    train_df = train.copy()\n",
    "\n",
    "    def _get_score_by_rc(input_df, fold):\n",
    "        info_lst = []\n",
    "        for rc in input_df[\"R-C\"].unique():\n",
    "            rc_df = input_df[input_df[\"R-C\"] == rc].reset_index(drop=True)\n",
    "            score = metrics(rc_df[\"pressure\"].values, rc_df[\"oof\"].values, mask=rc_df[\"u_out\"]==0)\n",
    "            info_lst.append([f\"fold={fold}\", rc, score])\n",
    "            \n",
    "        score = metrics(input_df[\"pressure\"].values, input_df[\"oof\"].values, mask=input_df[\"u_out\"]==0)\n",
    "        info_lst.append([f\"fold={fold}\", \"all\", score])\n",
    "        return info_lst\n",
    "\n",
    "    train_df[\"R-C\"] = train[\"R\"].astype(str) + \"-\" + train[\"C\"].astype(str)\n",
    "    plot_info = []\n",
    "    for f in train_df[\"fold\"].unique():\n",
    "        if f == -1:\n",
    "            continue\n",
    "        fold_df = train_df[train_df[\"fold\"] == f].reset_index(drop=True)\n",
    "        info_lst = _get_score_by_rc(fold_df, fold=f)\n",
    "        plot_info += info_lst\n",
    "    \n",
    "    info_lst =  _get_score_by_rc(train_df, fold=\"ALL\")\n",
    "    plot_info += info_lst\n",
    "        \n",
    "    plot_df = pd.DataFrame(plot_info, columns=[\"fold\", \"R-C\", \"MAE\"])\n",
    "    plot_df = plot_df.sort_values([\"fold\", \"R-C\"]).reset_index(drop=True)\n",
    "    fig = px.bar(plot_df, x=\"fold\", y=\"MAE\", color='R-C', barmode='group')\n",
    "\n",
    "    return plot_df, fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242b4ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T09:31:03.874790Z",
     "iopub.status.busy": "2021-11-04T09:31:03.874169Z",
     "iopub.status.idle": "2021-11-04T09:31:18.826141Z",
     "shell.execute_reply": "2021-11-04T09:31:18.825586Z",
     "shell.execute_reply.started": "2021-11-04T08:41:15.325147Z"
    },
    "papermill": {
     "duration": 15.080924,
     "end_time": "2021-11-04T09:31:18.826363",
     "exception": false,
     "start_time": "2021-11-04T09:31:03.745439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df, fig =  show_oof_score(train_)  \n",
    "fig.show()\n",
    "fig.write_html(os.path.join(EXP_FIG, \"oof_score.html\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080cc1dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T09:31:19.083893Z",
     "iopub.status.busy": "2021-11-04T09:31:19.083262Z",
     "iopub.status.idle": "2021-11-04T09:32:22.037214Z",
     "shell.execute_reply": "2021-11-04T09:32:22.037725Z",
     "shell.execute_reply.started": "2021-11-04T08:41:28.281264Z"
    },
    "papermill": {
     "duration": 63.086513,
     "end_time": "2021-11-04T09:32:22.037908",
     "exception": false,
     "start_time": "2021-11-04T09:31:18.951395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot bad predictions by R & C\n",
    "train_df_by_breath = (pd.merge(\n",
    "    pd.DataFrame(oof_score_by_breath, columns=[\"MAE\"]),\n",
    "    train_[[\"breath_id\", \"R\", \"C\"]].groupby(\"breath_id\").head(1),\n",
    "    on=\"breath_id\", how=\"left\"))\n",
    "train_df_by_breath[\"R-C\"] = train_df_by_breath[\"R\"].astype(str) + \"-\" + train_df_by_breath[\"C\"].astype(str)\n",
    "\n",
    "bad12_by_rc_dict = {}\n",
    "for rc in train_df_by_breath[\"R-C\"].unique():\n",
    "    _df = train_df_by_breath[train_df_by_breath[\"R-C\"] == rc].reset_index(drop=True)\n",
    "    breath_ids = _df.sort_values(\"MAE\", ascending=False)[\"breath_id\"].tolist()[:12]\n",
    "    bad12_by_rc_dict[rc] = breath_ids\n",
    "\n",
    "    fig = plot_pressure_line(train_, breath_ids=bad12_by_rc_dict[rc], additional_cols=[\"oof\"])\n",
    "    plt.plot()\n",
    "    fig.savefig(os.path.join(EXP_FIG, f\"bad_oof_rc_{rc}.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd6b1f",
   "metadata": {
    "papermill": {
     "duration": 0.198809,
     "end_time": "2021-11-04T09:32:22.438197",
     "exception": false,
     "start_time": "2021-11-04T09:32:22.239388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96494b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T09:32:22.839679Z",
     "iopub.status.busy": "2021-11-04T09:32:22.838953Z",
     "iopub.status.idle": "2021-11-04T09:32:37.534744Z",
     "shell.execute_reply": "2021-11-04T09:32:37.534235Z",
     "shell.execute_reply.started": "2021-11-04T08:42:24.362836Z"
    },
    "papermill": {
     "duration": 14.900101,
     "end_time": "2021-11-04T09:32:37.534898",
     "exception": false,
     "start_time": "2021-11-04T09:32:22.634797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(SUBMISSION, f\"{Config.name_v1}.csv\")\n",
    "sample_submission.to_csv(filepath, index=False)\n",
    "message = f\"train-fold-num:{len(Config.trn_fold)},oof-score:{oof_score:.5f},oof-score-all:{oof_score_all:.5f}\"\n",
    "logger.info(message)\n",
    "\n",
    "if Config.submit_from_colab:\n",
    "    ! kaggle competitions submit -c ventilator-pressure-prediction -f $filepath -m $message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7e683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T09:32:37.932294Z",
     "iopub.status.busy": "2021-11-04T09:32:37.931310Z",
     "iopub.status.idle": "2021-11-04T09:32:37.938894Z",
     "shell.execute_reply": "2021-11-04T09:32:37.939354Z"
    },
    "papermill": {
     "duration": 0.20895,
     "end_time": "2021-11-04T09:32:37.939520",
     "exception": false,
     "start_time": "2021-11-04T09:32:37.730570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload output folder to kaggle dataset\n",
    "if Config.upload_from_colab:\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name, upload_dir):\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n",
    "        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "        dataset_metadata['title'] = dataset_name\n",
    "        with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n",
    "\n",
    "    if len(EXP) >= 50:\n",
    "        dataset_name = EXP[:7]\n",
    "\n",
    "    dataset_create_new(dataset_name=dataset_name, upload_dir=OUTPUT_EXP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2673.607532,
   "end_time": "2021-11-04T09:32:40.975294",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-04T08:48:07.367762",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
